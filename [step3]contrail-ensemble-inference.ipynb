{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/ttach-image-test-time-augmentation/ttach-0.0.3-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:17:01.868196Z","iopub.execute_input":"2023-08-08T03:17:01.868560Z","iopub.status.idle":"2023-08-08T03:17:34.133687Z","shell.execute_reply.started":"2023-08-08T03:17:01.868532Z","shell.execute_reply":"2023-08-08T03:17:34.132446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /kaggle/tmp","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:17:34.136631Z","iopub.execute_input":"2023-08-08T03:17:34.137381Z","iopub.status.idle":"2023-08-08T03:17:35.079014Z","shell.execute_reply.started":"2023-08-08T03:17:34.137341Z","shell.execute_reply":"2023-08-08T03:17:35.077781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /kaggle/model","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:17:35.080550Z","iopub.execute_input":"2023-08-08T03:17:35.080943Z","iopub.status.idle":"2023-08-08T03:17:36.058323Z","shell.execute_reply.started":"2023-08-08T03:17:35.080904Z","shell.execute_reply":"2023-08-08T03:17:36.057021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/contrails-download-googledrive/*.ckpt /kaggle/model/","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:17:36.061293Z","iopub.execute_input":"2023-08-08T03:17:36.061965Z","iopub.status.idle":"2023-08-08T03:18:02.913403Z","shell.execute_reply.started":"2023-08-08T03:17:36.061925Z","shell.execute_reply":"2023-08-08T03:18:02.912129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/contrails-xavierp-contrails1-from-huggingface/Contrails1/*.ckpt /kaggle/model/","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:18:02.916041Z","iopub.execute_input":"2023-08-08T03:18:02.916356Z","iopub.status.idle":"2023-08-08T03:18:29.285061Z","shell.execute_reply.started":"2023-08-08T03:18:02.916328Z","shell.execute_reply":"2023-08-08T03:18:29.283674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/contrails-models-from-huggingface7/Contrails7/*.pth /kaggle/model/","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:18:29.287142Z","iopub.execute_input":"2023-08-08T03:18:29.287541Z","iopub.status.idle":"2023-08-08T03:18:58.830863Z","shell.execute_reply.started":"2023-08-08T03:18:29.287503Z","shell.execute_reply":"2023-08-08T03:18:58.829304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/contrails-models-from-huggingface8/Contrails8/*.pth /kaggle/model/","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:18:58.834015Z","iopub.execute_input":"2023-08-08T03:18:58.835035Z","iopub.status.idle":"2023-08-08T03:19:23.579531Z","shell.execute_reply.started":"2023-08-08T03:18:58.834993Z","shell.execute_reply":"2023-08-08T03:19:23.578063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/contrails-models-from-huggingface9/Contrails9/*.ckpt /kaggle/model/","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:19:23.581695Z","iopub.execute_input":"2023-08-08T03:19:23.582106Z","iopub.status.idle":"2023-08-08T03:19:29.493210Z","shell.execute_reply.started":"2023-08-08T03:19:23.582067Z","shell.execute_reply":"2023-08-08T03:19:29.491988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/contrails-models-from-huggingface3/Contrails6/unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth /kaggle/model/","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:19:29.495270Z","iopub.execute_input":"2023-08-08T03:19:29.495668Z","iopub.status.idle":"2023-08-08T03:19:40.651220Z","shell.execute_reply.started":"2023-08-08T03:19:29.495628Z","shell.execute_reply":"2023-08-08T03:19:40.649954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/contrails-models-from-huggingface/Contrails1/*.pth /kaggle/model/","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:19:40.656947Z","iopub.execute_input":"2023-08-08T03:19:40.659066Z","iopub.status.idle":"2023-08-08T03:20:01.860546Z","shell.execute_reply.started":"2023-08-08T03:19:40.659032Z","shell.execute_reply":"2023-08-08T03:20:01.859287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/contrails-models-from-huggingface/Contrails2/*.pth /kaggle/model/","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:01.862344Z","iopub.execute_input":"2023-08-08T03:20:01.862709Z","iopub.status.idle":"2023-08-08T03:20:42.685978Z","shell.execute_reply.started":"2023-08-08T03:20:01.862673Z","shell.execute_reply":"2023-08-08T03:20:42.684699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/contrails-models-from-huggingface10/Contrails10/*.ckpt /kaggle/model/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/contrails-models-from-huggingface2/Contrails3/*.pth /kaggle/model/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"configs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n#['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n['Unet_tu-seresnextaa101d_32x8d_864_dice671.ckpt', 0.52, 0.679],\n['Unet++_tu-seresnextaa101d_32x8d_768_2.ckpt', 0.52, 0.679],\n['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep5_dice0.6579.pth', 0.5, 0.657],\n#['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep5_dice0.6579.pth', 0.5, 0.657],\n          ]","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:42.689213Z","iopub.execute_input":"2023-08-08T03:20:42.689573Z","iopub.status.idle":"2023-08-08T03:20:42.701367Z","shell.execute_reply.started":"2023-08-08T03:20:42.689542Z","shell.execute_reply":"2023-08-08T03:20:42.700395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(configs)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:42.703920Z","iopub.execute_input":"2023-08-08T03:20:42.704249Z","iopub.status.idle":"2023-08-08T03:20:43.485708Z","shell.execute_reply.started":"2023-08-08T03:20:42.704217Z","shell.execute_reply":"2023-08-08T03:20:43.484741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con21.py\nK = 21\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n['Unet_tu-seresnextaa101d_32x8d_864_dice671.ckpt', 0.52, 0.679],\n['Unet++_tu-seresnextaa101d_32x8d_768_2.ckpt', 0.52, 0.679],\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con22.py\nK = 22\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n['Unet_tu-seresnextaa101d_32x8d_864_dice671.ckpt', 0.52, 0.679],\n['Unet++_tu-seresnextaa101d_32x8d_768_2.ckpt', 0.52, 0.679],\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con23.py\nK = 23\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n['Unet_tu-seresnextaa101d_32x8d_864_dice671.ckpt', 0.52, 0.679],\n['Unet++_tu-seresnextaa101d_32x8d_768_2.ckpt', 0.52, 0.679],\n['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep5_dice0.6579.pth', 0.5, 0.657],\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con0.py\nK = 0\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.487371Z","iopub.execute_input":"2023-08-08T03:20:43.487970Z","iopub.status.idle":"2023-08-08T03:20:43.504322Z","shell.execute_reply.started":"2023-08-08T03:20:43.487893Z","shell.execute_reply":"2023-08-08T03:20:43.503408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con1.py\nK = 1\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.505927Z","iopub.execute_input":"2023-08-08T03:20:43.506257Z","iopub.status.idle":"2023-08-08T03:20:43.525140Z","shell.execute_reply.started":"2023-08-08T03:20:43.506227Z","shell.execute_reply":"2023-08-08T03:20:43.524221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con2.py\nK = 2\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.526637Z","iopub.execute_input":"2023-08-08T03:20:43.527205Z","iopub.status.idle":"2023-08-08T03:20:43.546905Z","shell.execute_reply.started":"2023-08-08T03:20:43.527175Z","shell.execute_reply":"2023-08-08T03:20:43.545622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con3.py\nK = 3\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.548598Z","iopub.execute_input":"2023-08-08T03:20:43.549135Z","iopub.status.idle":"2023-08-08T03:20:43.566215Z","shell.execute_reply.started":"2023-08-08T03:20:43.549106Z","shell.execute_reply":"2023-08-08T03:20:43.564742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con4.py\nK = 4\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.568964Z","iopub.execute_input":"2023-08-08T03:20:43.569316Z","iopub.status.idle":"2023-08-08T03:20:43.585371Z","shell.execute_reply.started":"2023-08-08T03:20:43.569293Z","shell.execute_reply":"2023-08-08T03:20:43.584390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con5.py\nK = 5\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.587338Z","iopub.execute_input":"2023-08-08T03:20:43.587653Z","iopub.status.idle":"2023-08-08T03:20:43.604632Z","shell.execute_reply.started":"2023-08-08T03:20:43.587624Z","shell.execute_reply":"2023-08-08T03:20:43.603526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con6.py\nK = 6\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.606428Z","iopub.execute_input":"2023-08-08T03:20:43.606747Z","iopub.status.idle":"2023-08-08T03:20:43.624686Z","shell.execute_reply.started":"2023-08-08T03:20:43.606719Z","shell.execute_reply":"2023-08-08T03:20:43.623681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con7.py\nK = 7\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.626188Z","iopub.execute_input":"2023-08-08T03:20:43.626549Z","iopub.status.idle":"2023-08-08T03:20:43.644396Z","shell.execute_reply.started":"2023-08-08T03:20:43.626520Z","shell.execute_reply":"2023-08-08T03:20:43.643421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con8.py\nK = 8\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.646007Z","iopub.execute_input":"2023-08-08T03:20:43.646474Z","iopub.status.idle":"2023-08-08T03:20:43.661705Z","shell.execute_reply.started":"2023-08-08T03:20:43.646445Z","shell.execute_reply":"2023-08-08T03:20:43.660730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con9.py\nK = 9\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.663379Z","iopub.execute_input":"2023-08-08T03:20:43.664240Z","iopub.status.idle":"2023-08-08T03:20:43.685516Z","shell.execute_reply.started":"2023-08-08T03:20:43.664208Z","shell.execute_reply":"2023-08-08T03:20:43.684558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con10.py\nK = 10\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.687455Z","iopub.execute_input":"2023-08-08T03:20:43.688703Z","iopub.status.idle":"2023-08-08T03:20:43.703109Z","shell.execute_reply.started":"2023-08-08T03:20:43.688654Z","shell.execute_reply":"2023-08-08T03:20:43.702178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con11.py\nK = 11\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.704631Z","iopub.execute_input":"2023-08-08T03:20:43.705181Z","iopub.status.idle":"2023-08-08T03:20:43.723343Z","shell.execute_reply.started":"2023-08-08T03:20:43.705150Z","shell.execute_reply":"2023-08-08T03:20:43.722226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con12.py\nK = 12\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.725071Z","iopub.execute_input":"2023-08-08T03:20:43.725401Z","iopub.status.idle":"2023-08-08T03:20:43.741368Z","shell.execute_reply.started":"2023-08-08T03:20:43.725372Z","shell.execute_reply":"2023-08-08T03:20:43.740318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con13.py\nK = 13\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.743218Z","iopub.execute_input":"2023-08-08T03:20:43.743587Z","iopub.status.idle":"2023-08-08T03:20:43.761412Z","shell.execute_reply.started":"2023-08-08T03:20:43.743519Z","shell.execute_reply":"2023-08-08T03:20:43.760186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con14.py\nK = 14\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.770125Z","iopub.execute_input":"2023-08-08T03:20:43.770615Z","iopub.status.idle":"2023-08-08T03:20:43.785453Z","shell.execute_reply.started":"2023-08-08T03:20:43.770590Z","shell.execute_reply":"2023-08-08T03:20:43.784432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con15.py\nK = 15\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.787037Z","iopub.execute_input":"2023-08-08T03:20:43.787454Z","iopub.status.idle":"2023-08-08T03:20:43.803926Z","shell.execute_reply.started":"2023-08-08T03:20:43.787424Z","shell.execute_reply":"2023-08-08T03:20:43.803093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con16.py\nK = 16\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.805532Z","iopub.execute_input":"2023-08-08T03:20:43.806380Z","iopub.status.idle":"2023-08-08T03:20:43.833731Z","shell.execute_reply.started":"2023-08-08T03:20:43.806349Z","shell.execute_reply":"2023-08-08T03:20:43.831284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con17.py\nK = 17\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n#['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.836872Z","iopub.execute_input":"2023-08-08T03:20:43.837199Z","iopub.status.idle":"2023-08-08T03:20:43.917956Z","shell.execute_reply.started":"2023-08-08T03:20:43.837170Z","shell.execute_reply":"2023-08-08T03:20:43.916074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con18.py\nK = 18\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.920276Z","iopub.execute_input":"2023-08-08T03:20:43.925000Z","iopub.status.idle":"2023-08-08T03:20:43.992782Z","shell.execute_reply.started":"2023-08-08T03:20:43.924961Z","shell.execute_reply":"2023-08-08T03:20:43.990927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con19.py\nK = 19\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:43.994729Z","iopub.execute_input":"2023-08-08T03:20:43.995620Z","iopub.status.idle":"2023-08-08T03:20:44.115685Z","shell.execute_reply.started":"2023-08-08T03:20:43.995582Z","shell.execute_reply":"2023-08-08T03:20:44.114597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile con20.py\nK = 20\nconfigs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n\n          ]\n\nencoders = ['tu-tf_efficientnetv2_xl', \n            'tu-tf_efficientnetv2_l', \n            'timm-resnest200e',\n            'tu-seresnextaa101d_32x8d',\n            'timm-efficientnet-b7',\n            'tu-resnest269e',\n            'efficientnet-b7',\n            'tu-resnetrs420',\n            'timm-resnest269e'\n            ]\nno_tta_models = ['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth',\n                'unetpp_efficientnet-b7_768_ep15_dice0.6495.pth',\n                'unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth',\n                'unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth',\n                'unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth',]\n\ncurrent_model = configs[K]\nimport warnings \nwarnings.simplefilter('ignore')\nfrom pathlib import Path\nimport os\nimport random\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom tqdm.auto import tqdm\n\nimport ttach as tta\n\nimport sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nimport segmentation_models_pytorch as smp\n\n#print(f\"Segmentation Models version: {smp.__version__}\")\n\nclass Config:\n    batch_size = 8\n    seed = 42\n    thr = 0.53\n    \n    encoder = None\n    pretrained = False\n    weights = None\n    classes = ['contrail']\n    activation = None\n    in_chans = 3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    image_size = None\n    \n    ckpt = None\n\nclass Paths:\n    data = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test/'\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(Config.seed)\n\nif current_model[0] in no_tta_models:\n    tta_transforms = tta.Compose(\n                                    [\n                                        #tta.HorizontalFlip(),\n                                        tta.Rotate90(angles=[0]),       \n                                    ]\n                                                            )\n    print('no use tta.')\nelse:\n    \n    tta_transforms = tta.Compose(\n                                    [\n                                        tta.HorizontalFlip(),\n                                        #tta.VerticalFlip(),\n                                        tta.Rotate90(angles=[0, 90]),\n                                        #tta.Scale(scales=[1, 2, 4]),\n                                        #tta.Multiply(factors=[0.9, 1, 1.1]),        \n                                    ]\n                                )\n    print('use tta.')\nConfig.ckpt = f'/kaggle/model/{current_model[0]}'\nfor en in encoders:\n    if en in current_model[0]:\n        Config.encoder = en \n        break\nif '512' in current_model[0]:\n    Config.image_size = 512\n\nif '768' in current_model[0]:\n    Config.image_size = 768\n    \nif '864' in current_model[0]:\n    Config.image_size = 864  \n    \nif 'resnest269e' in current_model[0] or Config.image_size==864:\n    Config.batch_size = 4\n    \nprint('image_size:', Config.image_size)\nprint('batch_size:', Config.batch_size)\nfilenames = os.listdir(Paths.data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.data_root + test_df['record_id'].astype(str)\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size, train=True):\n        \n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize_image = T.transforms.Resize(image_size)\n        self.image_size = image_size\n        \n    def read_record(self, directory):\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        img = self.get_false_color(data)\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        \n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n            \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)\n\ntest_ds = ContrailsDataset(\n        test_df,\n        Config.image_size,\n        train = False\n    )\n \ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2, shuffle=False, drop_last=False,)\n\n## Inference\n\nclass UNet(nn.Module):\n    def __init__(self, cfg, path):\n        super(UNet, self).__init__()\n        \n        self.cfg = cfg\n        print(path)\n        if 'pp' in path or '++' in path:\n           seg_model = smp.UnetPlusPlus\n           print('use UnetPlusPlus.')\n        else:\n           seg_model =  smp.Unet \n           print('use Unet.')\n\n\n        self.model = seg_model(\n            encoder_name=cfg.encoder, \n            encoder_weights=cfg.weights, \n            decoder_use_batchnorm=True,\n            classes=len(cfg.classes), \n            activation=cfg.activation,\n        )\n        \n        self.model2  = tta.SegmentationTTAWrapper(self.model, tta_transforms, merge_mode= \"mean\",)\n        state_dict = torch.load(path)#['state_dict']\n        if 'state_dict' in state_dict:\n            state_dict = state_dict['state_dict']\n            print('use state_dict.')\n        self.model2.load_state_dict(state_dict)\n        \n    \n    def forward(self, imgs):\n        \n        x = imgs\n        logits = self.model2(x)\n        #logits2 = self.model3(x)\n        \n        if Config.image_size != 256:\n            logits = F.interpolate(logits, size=(256, 256), mode='bilinear')\n        \n        return {\"logits\": logits.sigmoid()}\n\nmodel = UNet(Config, Config.ckpt).to(Config.device)\nmodel.eval()\ntorch.set_grad_enabled(False)\nval_data = defaultdict(list)\npbar = tqdm(enumerate(test_dl), total=len(test_dl), desc='Test')\nfor step, X in pbar: \n    X = X.to(Config.device)\n\n    output = model(X)\n    for key, val in output.items():\n        val_data[key] += [output[key]]\n\nfor key, val in output.items():\n    value = val_data[key]\n    if len(value[0].shape) == 0:\n        val_data[key] = torch.stack(value)\n    else:\n        val_data[key] = torch.cat(value, dim=0).cpu().detach().numpy()\n\nimport pickle\nsubmission = pd.read_csv(Paths.data + '/sample_submission.csv', index_col='record_id')\nIds = []\nprediction = []\nfor i, pred in enumerate(val_data['logits']):\n    rec = test_df['record_id'][i]\n    Ids.append(rec)\n    prediction.append(pred[0])\n\nwith open(f'/kaggle/tmp/{current_model[0]}', 'wb') as f:\n    pickle.dump(prediction, f)\nwith open('/kaggle/tmp/ids1.pkl', 'wb') as f:\n    pickle.dump(Ids, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:44.118459Z","iopub.execute_input":"2023-08-08T03:20:44.119260Z","iopub.status.idle":"2023-08-08T03:20:44.259632Z","shell.execute_reply.started":"2023-08-08T03:20:44.119219Z","shell.execute_reply":"2023-08-08T03:20:44.258689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con0.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:20:44.261200Z","iopub.execute_input":"2023-08-08T03:20:44.261793Z","iopub.status.idle":"2023-08-08T03:21:34.561833Z","shell.execute_reply.started":"2023-08-08T03:20:44.261736Z","shell.execute_reply":"2023-08-08T03:21:34.560624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con1.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:21:34.563979Z","iopub.execute_input":"2023-08-08T03:21:34.564360Z","iopub.status.idle":"2023-08-08T03:22:07.849736Z","shell.execute_reply.started":"2023-08-08T03:21:34.564321Z","shell.execute_reply":"2023-08-08T03:22:07.848022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con2.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:22:07.853931Z","iopub.execute_input":"2023-08-08T03:22:07.854432Z","iopub.status.idle":"2023-08-08T03:22:42.556281Z","shell.execute_reply.started":"2023-08-08T03:22:07.854378Z","shell.execute_reply":"2023-08-08T03:22:42.554927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con3.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:22:42.559333Z","iopub.execute_input":"2023-08-08T03:22:42.559751Z","iopub.status.idle":"2023-08-08T03:23:09.892343Z","shell.execute_reply.started":"2023-08-08T03:22:42.559707Z","shell.execute_reply":"2023-08-08T03:23:09.891169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con4.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:23:09.894107Z","iopub.execute_input":"2023-08-08T03:23:09.894795Z","iopub.status.idle":"2023-08-08T03:23:47.521465Z","shell.execute_reply.started":"2023-08-08T03:23:09.894731Z","shell.execute_reply":"2023-08-08T03:23:47.520296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con5.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:23:47.525232Z","iopub.execute_input":"2023-08-08T03:23:47.525544Z","iopub.status.idle":"2023-08-08T03:24:14.981503Z","shell.execute_reply.started":"2023-08-08T03:23:47.525514Z","shell.execute_reply":"2023-08-08T03:24:14.980004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con6.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:24:14.983259Z","iopub.execute_input":"2023-08-08T03:24:14.983642Z","iopub.status.idle":"2023-08-08T03:24:49.389589Z","shell.execute_reply.started":"2023-08-08T03:24:14.983605Z","shell.execute_reply":"2023-08-08T03:24:49.388424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con7.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:24:49.391871Z","iopub.execute_input":"2023-08-08T03:24:49.392987Z","iopub.status.idle":"2023-08-08T03:25:24.936302Z","shell.execute_reply.started":"2023-08-08T03:24:49.392944Z","shell.execute_reply":"2023-08-08T03:25:24.934867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con8.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:25:24.938219Z","iopub.execute_input":"2023-08-08T03:25:24.938700Z","iopub.status.idle":"2023-08-08T03:25:55.170970Z","shell.execute_reply.started":"2023-08-08T03:25:24.938650Z","shell.execute_reply":"2023-08-08T03:25:55.169775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con9.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:25:55.173140Z","iopub.execute_input":"2023-08-08T03:25:55.173521Z","iopub.status.idle":"2023-08-08T03:26:26.084828Z","shell.execute_reply.started":"2023-08-08T03:25:55.173482Z","shell.execute_reply":"2023-08-08T03:26:26.083611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con10.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:26:26.087016Z","iopub.execute_input":"2023-08-08T03:26:26.087437Z","iopub.status.idle":"2023-08-08T03:26:52.218522Z","shell.execute_reply.started":"2023-08-08T03:26:26.087398Z","shell.execute_reply":"2023-08-08T03:26:52.217356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con11.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:26:52.220432Z","iopub.execute_input":"2023-08-08T03:26:52.220803Z","iopub.status.idle":"2023-08-08T03:27:23.905077Z","shell.execute_reply.started":"2023-08-08T03:26:52.220771Z","shell.execute_reply":"2023-08-08T03:27:23.903830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con12.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:27:23.907920Z","iopub.execute_input":"2023-08-08T03:27:23.908659Z","iopub.status.idle":"2023-08-08T03:27:55.168887Z","shell.execute_reply.started":"2023-08-08T03:27:23.908612Z","shell.execute_reply":"2023-08-08T03:27:55.167711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con13.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:27:55.170752Z","iopub.execute_input":"2023-08-08T03:27:55.171158Z","iopub.status.idle":"2023-08-08T03:28:22.691786Z","shell.execute_reply.started":"2023-08-08T03:27:55.171125Z","shell.execute_reply":"2023-08-08T03:28:22.690577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con14.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:28:22.695022Z","iopub.execute_input":"2023-08-08T03:28:22.695881Z","iopub.status.idle":"2023-08-08T03:28:54.974953Z","shell.execute_reply.started":"2023-08-08T03:28:22.695848Z","shell.execute_reply":"2023-08-08T03:28:54.973727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con15.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:28:54.977168Z","iopub.execute_input":"2023-08-08T03:28:54.977601Z","iopub.status.idle":"2023-08-08T03:29:21.300958Z","shell.execute_reply.started":"2023-08-08T03:28:54.977557Z","shell.execute_reply":"2023-08-08T03:29:21.299407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con16.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:29:21.304260Z","iopub.execute_input":"2023-08-08T03:29:21.304575Z","iopub.status.idle":"2023-08-08T03:29:53.001551Z","shell.execute_reply.started":"2023-08-08T03:29:21.304545Z","shell.execute_reply":"2023-08-08T03:29:53.000354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con17.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:29:53.004835Z","iopub.execute_input":"2023-08-08T03:29:53.005146Z","iopub.status.idle":"2023-08-08T03:30:27.073337Z","shell.execute_reply.started":"2023-08-08T03:29:53.005115Z","shell.execute_reply":"2023-08-08T03:30:27.072175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con18.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:30:27.074910Z","iopub.execute_input":"2023-08-08T03:30:27.075314Z","iopub.status.idle":"2023-08-08T03:30:54.352964Z","shell.execute_reply.started":"2023-08-08T03:30:27.075255Z","shell.execute_reply":"2023-08-08T03:30:54.351781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con19.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:30:54.355043Z","iopub.execute_input":"2023-08-08T03:30:54.355421Z","iopub.status.idle":"2023-08-08T03:31:28.824103Z","shell.execute_reply.started":"2023-08-08T03:30:54.355384Z","shell.execute_reply":"2023-08-08T03:31:28.822831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con20.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:31:28.825972Z","iopub.execute_input":"2023-08-08T03:31:28.826364Z","iopub.status.idle":"2023-08-08T03:32:02.033845Z","shell.execute_reply.started":"2023-08-08T03:31:28.826324Z","shell.execute_reply":"2023-08-08T03:32:02.032616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con21.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:32:02.035816Z","iopub.execute_input":"2023-08-08T03:32:02.036195Z","iopub.status.idle":"2023-08-08T03:32:03.092031Z","shell.execute_reply.started":"2023-08-08T03:32:02.036157Z","shell.execute_reply":"2023-08-08T03:32:03.090789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con22.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:32:02.035816Z","iopub.execute_input":"2023-08-08T03:32:02.036195Z","iopub.status.idle":"2023-08-08T03:32:03.092031Z","shell.execute_reply.started":"2023-08-08T03:32:02.036157Z","shell.execute_reply":"2023-08-08T03:32:03.090789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python con23.py","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:32:02.035816Z","iopub.execute_input":"2023-08-08T03:32:02.036195Z","iopub.status.idle":"2023-08-08T03:32:03.092031Z","shell.execute_reply.started":"2023-08-08T03:32:02.036157Z","shell.execute_reply":"2023-08-08T03:32:03.090789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_encode(x, fg_val=1):\n    \"\"\"\n    Args:\n        x:  numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns: run length encoding as list\n    \"\"\"\n\n    dots = np.where(\n        x.T.flatten() == fg_val)[0]  # .T sets Fortran order down-then-right\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if b > prev + 1:\n            run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\ndef list_to_string(x):\n    \"\"\"\n    Converts list to a string representation\n    Empty list returns '-'\n    \"\"\"\n    if x: # non-empty list\n        s = str(x).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")\n    else:\n        s = '-'\n    return s","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:32:03.094433Z","iopub.execute_input":"2023-08-08T03:32:03.094867Z","iopub.status.idle":"2023-08-08T03:32:03.105716Z","shell.execute_reply.started":"2023-08-08T03:32:03.094826Z","shell.execute_reply":"2023-08-08T03:32:03.103830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:32:03.107565Z","iopub.execute_input":"2023-08-08T03:32:03.107927Z","iopub.status.idle":"2023-08-08T03:32:03.117601Z","shell.execute_reply.started":"2023-08-08T03:32:03.107897Z","shell.execute_reply":"2023-08-08T03:32:03.114729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/tmp/ids1.pkl', 'rb') as f:\n    Ids = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:32:03.119515Z","iopub.execute_input":"2023-08-08T03:32:03.120245Z","iopub.status.idle":"2023-08-08T03:32:03.127736Z","shell.execute_reply.started":"2023-08-08T03:32:03.120213Z","shell.execute_reply":"2023-08-08T03:32:03.126818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"configs = [['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep17_dice0.6738_aug2.pth', 0.52, 0.680],\n['Unet_tu-tf_efficientnetv2_xl_768_dice672.ckpt', 0.52, 0.678],\n#['Unet++_tu-tf_efficientnetv2_l_768_dice666.ckpt', 0.51, 0.671],\n['Unet_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.51, 0.679],\n['Unet_timm-resnest200e_size768_dice664.ckpt', 0.53, 0.670],\n#['unetpp_fl_timm-efficientnet-b7_768_ep5_dice0.6610_aug2.pth', 0.52, 0.669],\n['unetpp_fl_tu-resnest269e_768_ep13_dice0.6687_aug2.pth', 0.51, 0.674],\n['unetpp_fl_efficientnet-b7_768_ep17_dice0.6683_aug2.pth', 0.52, 0.671],\n#['unetpp_fl_tu-resnetrs420_768_ep22_dice0.6651_aug2.pth', 0.5, 0.668],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6696_aug2.pth', 0.51, 0.680],\n['Unet++_tu-tf_efficientnetv2_xl_768_dice671.ckpt', 0.52, 0.676],\n['Unet_timm-resnest269e_768_dice669.ckpt', 0.52, 0.672],\n['Unet_tu-tf_efficientnetv2_xl_512_dice661.ckpt', 0.53, 0.666],\n['Unet_tu-tf_efficientnetv2_l_size512_dice663.ckpt', 0.52, 0.670],\n['Unet++_timm-resnest269e_512_dice665.ckpt', 0.53, 0.671],\n#['unetpp_fl_tu-resnest269e_512_ep17_dice0.6653_aug2.pth', 0.51, 0.672],\n['unetpp_fl_tu-tf_efficientnetv2_xl_512_ep11_dice0.6624_aug2.pth', 0.52, 0.672],\n['unetpp_fl_tu-seresnextaa101d_32x8d_512_ep15_dice0.6678_aug2.pth', 0.51, 0.676],\n['unetpp_fl_tu-resnetrs420_512_ep16_dice0.6698_aug2.pth', 0.52, 0.672],\n['Unet_timm-efficientnet-b7_768.ckpt', 0.53, 0.673],\n['Unet++_tu-seresnextaa101d_32x8d_768.ckpt', 0.53, 0.675],\n\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep21_dice0.6532_aug.pth', 0.5, 0.653],\n           \n#['unetpp_efficientnet-b7_768_ep15_dice0.6495.pth', 0.5, 0.649],\n['unetpp_timm-efficientnet-b7_768_ep15_dice0.6540.pth', 0.5, 0.654],\n['unetpp_fl_tu-tf_efficientnetv2_xl_768_ep5_dice0.6562.pth', 0.5, 0.656],\n#['unetpp_fl_tu-resnetrs420_768_ep9_dice0.6503.pth', 0.5, 0.650],\n           \n['Unet_tu-tf_efficientnetv2_xl_864_dice672.ckpt', 0.52, 0.682],\n['Unet_tu-seresnextaa101d_32x8d_864_dice671.ckpt', 0.52, 0.679],\n['Unet++_tu-seresnextaa101d_32x8d_768_2.ckpt', 0.52, 0.679],\n['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep5_dice0.6579.pth', 0.5, 0.657],\n#['unetpp_fl_tu-seresnextaa101d_32x8d_768_ep5_dice0.6579.pth', 0.5, 0.657],\n          ]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = 0\nws = []\nfor k in configs:\n    with open(f'/kaggle/tmp/{k[0]}', 'rb') as f:\n        pred = pickle.load(f)\n    pred = np.stack(pred)\n    w = np.max([(k[2]-0.663)/(0.68-0.666), 0.1])\n    print(w)\n    predictions += pred*w\n    ws.append(w)\npredictions = predictions / np.sum(ws)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:32:03.130943Z","iopub.execute_input":"2023-08-08T03:32:03.131199Z","iopub.status.idle":"2023-08-08T03:32:03.214736Z","shell.execute_reply.started":"2023-08-08T03:32:03.131178Z","shell.execute_reply":"2023-08-08T03:32:03.213646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/google-research-identify-contrails-reduce-global-warming/sample_submission.csv', index_col='record_id')\ndef ensemble(predictions):\n    thr = 0.5\n    ps = (predictions > thr).astype(np.int32)\n    return ps\nfor q in range(len(Ids)):\n    img_id = Ids[q]\n    p1 = predictions[q]\n    submission.loc[int(img_id), 'encoded_pixels'] = list_to_string(rle_encode(ensemble(p1)))\n","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:32:03.216371Z","iopub.execute_input":"2023-08-08T03:32:03.216779Z","iopub.status.idle":"2023-08-08T03:32:03.249804Z","shell.execute_reply.started":"2023-08-08T03:32:03.216731Z","shell.execute_reply":"2023-08-08T03:32:03.248893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:32:03.251248Z","iopub.execute_input":"2023-08-08T03:32:03.251607Z","iopub.status.idle":"2023-08-08T03:32:03.259706Z","shell.execute_reply.started":"2023-08-08T03:32:03.251575Z","shell.execute_reply":"2023-08-08T03:32:03.258596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:32:03.261123Z","iopub.execute_input":"2023-08-08T03:32:03.261569Z","iopub.status.idle":"2023-08-08T03:32:03.275434Z","shell.execute_reply.started":"2023-08-08T03:32:03.261535Z","shell.execute_reply":"2023-08-08T03:32:03.274036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}